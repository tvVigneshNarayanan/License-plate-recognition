{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, TimeDistributed\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize data\n",
    "def load_images_and_annotations(images_path, annotations_file, is_bounding_box=True):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    annotation_data = pd.read_csv(annotations_file)\n",
    "    \n",
    "    # Ensure the 'img_id' column is correctly interpreted as string\n",
    "    annotation_data['img_id'] = annotation_data['img_id'].astype(str)\n",
    "    \n",
    "    for img_file in os.listdir(images_path):\n",
    "        img_path = os.path.join(images_path, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:  # Ensure the image is correctly loaded\n",
    "            images.append(img)\n",
    "            \n",
    "            if is_bounding_box:\n",
    "                ann = annotation_data[annotation_data['img_id'] == img_file]\n",
    "                if not ann.empty:\n",
    "                    ann = ann.iloc[0]\n",
    "                    annotations.append([ann['ymin'], ann['xmin'], ann['ymax'], ann['xmax']])\n",
    "                else:\n",
    "                    print(f\"No annotation found for image {img_file}\")\n",
    "                    annotations.append([0, 0, 0, 0])  # Dummy annotation if not found\n",
    "            else:\n",
    "                ann = annotation_data[annotation_data['img_id'] == img_file]\n",
    "                if not ann.empty:\n",
    "                    ann = ann.iloc[0]\n",
    "                    annotations.append(ann['text'])\n",
    "                else:\n",
    "                    print(f\"No annotation found for image {img_file}\")\n",
    "                    annotations.append(\"\")  # Dummy text if not found\n",
    "        else:\n",
    "            print(f\"Failed to load image {img_path}\")\n",
    "    \n",
    "    return np.array(images), np.array(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_annotations(train_set_path, train_set_annotations_file):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    \n",
    "    # Load annotations from file or database, assuming annotations are in a structured format\n",
    "    # For example, if annotations are in a CSV file:\n",
    "    # annotations_data = load_annotations_from_csv(train_set_annotations_file)\n",
    "    \n",
    "    # Assuming annotations_data is a list of tuples or dictionaries containing bounding box info\n",
    "    \n",
    "    # Load images\n",
    "    for img_path in train_set_path:\n",
    "        img = cv2.imread(img_path)  # Read image using OpenCV (adjust path as per your setup)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            # Assuming annotations are loaded in a compatible format (e.g., bounding boxes)\n",
    "            annotation = load_annotation_for_image(img_path, annotations_data)  # Adjust function as per your annotations format\n",
    "            annotations.append(annotation)\n",
    "        else:\n",
    "            print(f\"Failed to load image {img_path}\")\n",
    "    \n",
    "    # Convert lists to NumPy arrays\n",
    "    images = np.array(images)\n",
    "    annotations = np.array(annotations)\n",
    "    \n",
    "    return images, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def load_images_and_annotations(train_set_path, train_set_annotations_file):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    \n",
    "    # Load images with error handling\n",
    "    for img_filename in os.listdir(train_set_path):\n",
    "        img_path = os.path.join(train_set_path, img_filename)\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                # Load corresponding annotations based on your file format\n",
    "                # annotations.append(load_annotation(img_filename, train_set_annotations_file))\n",
    "            else:\n",
    "                print(f\"Failed to load image {img_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_filename}: {e}\")\n",
    "    \n",
    "    # Convert lists to NumPy arrays if needed\n",
    "    images = np.array(images)\n",
    "    # Convert annotations to appropriate format\n",
    "    \n",
    "    return images, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def load_images_and_annotations(train_set_path, train_set_annotations_file):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    \n",
    "    for img_filename in os.listdir(train_set_path):\n",
    "        img_path = os.path.join(train_set_path, img_filename)\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                # Perform preprocessing (e.g., resize, normalize)\n",
    "                img = cv2.resize(img, (desired_width, desired_height))\n",
    "                img = img.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "                \n",
    "                images.append(img)\n",
    "                print(f\"Loaded image {img_filename} with shape {img.shape}\")\n",
    "                \n",
    "                # Load corresponding annotations based on your file format\n",
    "                # annotations.append(load_annotation(img_filename, train_set_annotations_file))\n",
    "            else:\n",
    "                print(f\"Failed to load image {img_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_filename}: {e}\")\n",
    "    \n",
    "    # Convert lists to NumPy arrays if needed\n",
    "    images = np.array(images)\n",
    "    # Convert annotations to appropriate format\n",
    "    \n",
    "    return images, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def load_images_and_annotations(train_set_path, train_set_annotations_file):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    \n",
    "    desired_width = 224  # Example width for resizing\n",
    "    desired_height = 224  # Example height for resizing\n",
    "    \n",
    "    for img_filename in os.listdir(train_set_path):\n",
    "        img_path = os.path.join(train_set_path, img_filename)\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                # Perform resizing to desired dimensions\n",
    "                img = cv2.resize(img, (desired_width, desired_height))\n",
    "                img = img.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "                \n",
    "                images.append(img)\n",
    "                print(f\"Loaded image {img_filename} with shape {img.shape}\")\n",
    "                \n",
    "                # Load corresponding annotations based on your file format\n",
    "                # annotations.append(load_annotation(img_filename, train_set_annotations_file))\n",
    "            else:\n",
    "                print(f\"Failed to load image {img_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_filename}: {e}\")\n",
    "    \n",
    "    # Convert lists to NumPy arrays if needed\n",
    "    images = np.array(images)\n",
    "    # Convert annotations to appropriate format\n",
    "    \n",
    "    return images, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "train_set_1_path = 'C:\\\\Users\\\\Vignesh T\\\\Downloads\\\\Licplatesdetection_train\\\\license_plates_detection_train'\n",
    "train_set_1_annotations_file = 'C:\\\\Users\\\\Vignesh T\\\\Downloads\\\\Licplatesdetection_train.csv'\n",
    "train_set_2_path = 'C:\\\\Users\\\\Vignesh T\\\\Downloads\\\\Licplatesrecognition_train\\\\license_plates_recognition_train'\n",
    "train_set_2_annotations_file = 'C:\\\\Users\\\\Vignesh T\\\\Downloads\\\\Licplatesrecognition_train.csv'\n",
    "test_set_path = 'C:\\\\Users\\\\Vignesh T\\\\Downloads\\\\Licplatesdetection_test\\\\license_plates_detection_test'\n",
    "test_set_annotations_file = 'C:\\\\Users\\\\Vignesh T\\\\Downloads\\\\Licplatesdetection_test.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Set 1\n",
    "train_set_1_images, train_set_1_annotations = load_images_and_annotations(train_set_1_path, train_set_1_annotations_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Set 2\n",
    "train_set_2_images, train_set_2_annotations = load_images_and_annotations(train_set_2_path, train_set_2_annotations_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "def visualize_samples(images, annotations, is_bounding_box=True):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(min(5, len(images))):\n",
    "        img = images[i]\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        if is_bounding_box:\n",
    "            ymin, xmin, ymax, xmax = annotations[i]\n",
    "            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n",
    "            plt.title('Bounding Box')\n",
    "        else:\n",
    "            plt.title(f'Text: {annotations[i]}')\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def visualize_samples(images, annotations, is_bounding_box=True):\n",
    "    num_samples = len(images)\n",
    "    num_cols = 5  # Number of columns for subplots, adjust as needed\n",
    "    \n",
    "    # Calculate number of rows needed based on number of samples\n",
    "    num_rows = (num_samples // num_cols) + (1 if num_samples % num_cols != 0 else 0)\n",
    "    \n",
    "    plt.figure(figsize=(15, 3*num_rows))  # Adjust figure size as needed\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        img = images[i]\n",
    "        plt.subplot(num_rows, num_cols, i+1)\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        if i < len(annotations):  # Check if annotation exists for this image\n",
    "            if is_bounding_box:\n",
    "                ymin, xmin, ymax, xmax = annotations[i]\n",
    "                cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n",
    "                plt.title('Bounding Box')\n",
    "        else:\n",
    "            plt.title('No Annotation')  # Optional: Handle cases with no annotation\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage assuming train_set_1_images and train_set_1_annotations are defined\n",
    "visualize_samples(train_set_1_images, train_set_1_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_samples(train_set_1_images, train_set_1_annotations)\n",
    "visualize_samples(train_set_2_images, train_set_2_annotations, is_bounding_box=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def preprocess_images(images, target_size=(224, 224)):\n",
    "    processed_images = []\n",
    "    for img in images:\n",
    "        img_resized = cv2.resize(img, target_size)\n",
    "        img_normalized = img_resized / 255.0\n",
    "        processed_images.append(img_normalized)\n",
    "    return np.array(processed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the images\n",
    "train_set_1_images = preprocess_images(train_set_1_images)\n",
    "train_set_2_images = preprocess_images(train_set_2_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Detection Model\n",
    "def build_detection_model():\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(4, activation='linear')(x)  # 4 outputs for bounding box coordinates\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "detection_model = build_detection_model()\n",
    "detection_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Recognition Model\n",
    "def build_recognition_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(36, activation='softmax'))  # Assuming 36 classes: 10 digits + 26 letters\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "recognition_model = build_recognition_model()\n",
    "recognition_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example data (Replace these with your actual datasets)\n",
    "train_set_1_images = np.random.rand(100, 224, 224, 3)  # 100 images of shape 224x224 with 3 channels\n",
    "train_set_1_annotations = np.random.rand(100, 14)  # 100 annotations with 14 values (10 classes + 4 bounding box coordinates)\n",
    "\n",
    "# If they are NumPy arrays\n",
    "print(\"Shape of train_set_1_images (NumPy):\", train_set_1_images.shape)\n",
    "print(\"Shape of train_set_1_annotations (NumPy):\", train_set_1_annotations.shape)\n",
    "\n",
    "# If they are TensorFlow tensors\n",
    "train_set_1_images_tf = tf.convert_to_tensor(train_set_1_images)\n",
    "train_set_1_annotations_tf = tf.convert_to_tensor(train_set_1_annotations)\n",
    "\n",
    "print(\"Shape of train_set_1_images (TensorFlow):\", tf.shape(train_set_1_images_tf))\n",
    "print(\"Shape of train_set_1_annotations (TensorFlow):\", tf.shape(train_set_1_annotations_tf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    # Split the true and predicted values\n",
    "    y_true_class = y_true[:, :10]\n",
    "    y_true_bbox = y_true[:, 10:]\n",
    "    \n",
    "    y_pred_class = y_pred[:, :10]\n",
    "    y_pred_bbox = y_pred[:, 10:]\n",
    "\n",
    "    # Classification loss\n",
    "    classification_loss = tf.keras.losses.CategoricalCrossentropy()(y_true_class, y_pred_class)\n",
    "    \n",
    "    # Bounding box regression loss\n",
    "    bbox_loss = tf.keras.losses.MeanSquaredError()(y_true_bbox, y_pred_bbox)\n",
    "    \n",
    "    # Combine losses\n",
    "    total_loss = classification_loss + bbox_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_detection_loss(y_true, y_pred):\n",
    "    # Assuming y_true and y_pred have specific shapes for object detection task\n",
    "    # y_true: (batch_size, num_boxes, 5), where 5 is (class_label, x_min, y_min, x_max, y_max)\n",
    "    # y_pred: (batch_size, num_boxes, 5), same as y_true\n",
    "    \n",
    "    # Localization loss (smooth L1 loss)\n",
    "    localization_loss = tf.keras.losses.Huber()(y_true[..., 1:], y_pred[..., 1:])  \n",
    "    \n",
    "    # Classification loss (sparse categorical cross-entropy)\n",
    "    classification_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true[..., 0], y_pred[..., 0])  \n",
    "    \n",
    "    return localization_loss + classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the character annotations for Training Set 2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_set_2_annotations_encoded = le.fit_transform(train_set_2_annotations)\n",
    "train_set_2_annotations_one_hot = to_categorical(train_set_2_annotations_encoded, num_classes=36)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example input shape\n",
    "input_shape = (32, 32, 3)  # Assuming 32x32 RGB images\n",
    "\n",
    "# Define your recognition model with appropriate input shape\n",
    "recognition_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # Example output layer with 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model with appropriate loss and metrics\n",
    "recognition_model.compile(optimizer='adam',\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary to verify the input shape\n",
    "recognition_model.summary()\n",
    "\n",
    "# Now, fit the model with your data\n",
    "recognition_model.fit(\n",
    "    train_set_2_images,\n",
    "    train_set_2_annotations_one_hot,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_set_2_images are loaded and need resizing\n",
    "from tensorflow.image import resize\n",
    "\n",
    "train_set_2_images_resized = resize(train_set_2_images, (32, 32))  # Resize images to 32x32\n",
    "\n",
    "# Check the shape after resizing\n",
    "print(train_set_2_images_resized.shape)  # Should output (batch_size, 32, 32, 3)\n",
    "\n",
    "# Now, fit the model with your resized data\n",
    "recognition_model.fit(\n",
    "    train_set_2_images_resized,\n",
    "    train_set_2_annotations_one_hot,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "import os\n",
    "\n",
    "def evaluate_models(test_set_path, test_set_annotations_file):\n",
    "    # Check if paths are correctly passed\n",
    "    print(\"test_set_path:\", test_set_path)\n",
    "    print(\"test_set_annotations_file:\", test_set_annotations_file)\n",
    "    \n",
    "    # Load images and annotations\n",
    "    test_images, test_annotations = load_images_and_annotations(test_set_path, test_set_annotations_file)\n",
    "    test_images = preprocess_images(test_images)\n",
    "    \n",
    "    # Object Detection Evaluation\n",
    "    predicted_boxes = detection_model.predict(test_images)\n",
    "    # Compute IoU and other metrics\n",
    "    \n",
    "    # Character Recognition Evaluation\n",
    "    # Assuming we have the ground truth annotations for the test set\n",
    "    # Compute character recognition accuracy\n",
    "\n",
    "# Example usage\n",
    "test_set_path = 'C:\\\\Users\\\\Vignesh T\\\\Downloads\\\\test\\\\test\\\\test'\n",
    "test_set_annotations_file = 'C:\\\\Users\\\\Vignesh T\\\\Downloads\\\\test\\\\test\\\\test'\n",
    "\n",
    "# Call the function with corrected paths\n",
    "evaluate_models(test_set_path, test_set_annotations_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def end_to_end_evaluation(test_set_path, test_set_annotations_file):\n",
    "    test_images, test_annotations = load_images_and_annotations(test_set_path, test_set_annotations_file)\n",
    "    \n",
    "    for img in test_images:\n",
    "        try:\n",
    "            img_preprocessed = preprocess_images([img])\n",
    "            predicted_boxes = detection_model.predict(img_preprocessed)\n",
    "            \n",
    "            for predicted_box in predicted_boxes:\n",
    "                if len(predicted_box) == 4:  # Check if predicted_box has the expected length\n",
    "                    ymin, xmin, ymax, xmax = predicted_box  # Unpack the coordinates\n",
    "                    ymin, xmin, ymax, xmax = int(ymin), int(xmin), int(ymax), int(xmax)  # Convert to integers\n",
    "                    \n",
    "                    # Ensure the box coordinates are within image bounds\n",
    "                    ymin = max(0, ymin)\n",
    "                    xmin = max(0, xmin)\n",
    "                    ymax = min(img.shape[0], ymax)\n",
    "                    xmax = min(img.shape[1], xmax)\n",
    "                    \n",
    "                    license_plate_img = img[ymin:ymax, xmin:xmax]\n",
    "                    \n",
    "                    # Ensure the license_plate_img is not empty\n",
    "                    if license_plate_img.size == 0:\n",
    "                        print(f\"Empty license plate image extracted from {img.shape} at ymin={ymin}, xmin={xmin}, ymax={ymax}, xmax={xmax}\")\n",
    "                        continue\n",
    "                    \n",
    "                    license_plate_img = preprocess_images([license_plate_img])\n",
    "                    \n",
    "                    predicted_characters = recognition_model.predict(license_plate_img)\n",
    "                    # Decode predicted characters and compare with ground truth\n",
    "                else:\n",
    "                    print(\"Unexpected format for predicted_box:\", predicted_box)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {e}\")\n",
    "\n",
    "# Example usage\n",
    "test_set_path = 'C:\\\\Users\\\\Vignesh T\\\\Downloads\\\\test\\\\test\\\\test'\n",
    "test_set_annotations_file = \"C:\\\\Users\\\\Vignesh T\\\\Downloads\\\\test_labels.csv\"\n",
    "\n",
    "# Call the function with corrected paths\n",
    "end_to_end_evaluation(test_set_path, test_set_annotations_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_to_end_evaluation(test_set_path, test_set_annotations_file):\n",
    "    test_images, test_annotations = load_images_and_annotations(test_set_path, test_set_annotations_file)\n",
    "    total_characters = 0\n",
    "    correct_characters = 0\n",
    "    images_processed = 0\n",
    "    plates_detected = False\n",
    "    \n",
    "    for img, annotation in zip(test_images, test_annotations):\n",
    "        try:\n",
    "            img_preprocessed = preprocess_images([img])\n",
    "            predicted_boxes = detection_model.predict(img_preprocessed)\n",
    "            \n",
    "            if len(predicted_boxes) == 0:\n",
    "                print(f\"No license plate detected in image {img.shape}\")\n",
    "                continue\n",
    "            \n",
    "            plates_detected = True  # Flag indicating at least one plate detected\n",
    "            \n",
    "            for predicted_box in predicted_boxes:\n",
    "                if len(predicted_box) == 4:\n",
    "                    ymin, xmin, ymax, xmax = predicted_box\n",
    "                    ymin, xmin, ymax, xmax = int(ymin), int(xmin), int(ymax), int(xmax)\n",
    "                    \n",
    "                    ymin = max(0, ymin)\n",
    "                    xmin = max(0, xmin)\n",
    "                    ymax = min(img.shape[0], ymax)\n",
    "                    xmax = min(img.shape[1], xmax)\n",
    "                    \n",
    "                    license_plate_img = img[ymin:ymax, xmin:xmax]\n",
    "                    \n",
    "                    if license_plate_img.size == 0:\n",
    "                        print(f\"Empty license plate image extracted from {img.shape} at ymin={ymin}, xmin={xmin}, ymax={ymax}, xmax={xmax}\")\n",
    "                        continue\n",
    "                    \n",
    "                    license_plate_img = preprocess_images([license_plate_img])\n",
    "                    \n",
    "                    predicted_characters = recognition_model.predict(license_plate_img)\n",
    "                    predicted_characters = decode_characters(predicted_characters)  # Implement decoding function\n",
    "                    \n",
    "                    ground_truth_characters = annotation['characters']  # Assuming 'characters' key in annotations\n",
    "                    ground_truth_characters = decode_characters(ground_truth_characters)  # Implement decoding function\n",
    "                    \n",
    "                    total_characters += len(ground_truth_characters)\n",
    "                    correct_characters += sum(1 for pc, gc in zip(predicted_characters, ground_truth_characters) if pc == gc)\n",
    "                    images_processed += 1\n",
    "                    \n",
    "                    # Optional: Print predicted vs ground truth for debugging\n",
    "                    print(f\"Predicted: {predicted_characters}, Ground Truth: {ground_truth_characters}\")\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Unexpected format for predicted_box:\", predicted_box)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {e}\")\n",
    "    \n",
    "    if plates_detected and images_processed > 0:\n",
    "        accuracy = (correct_characters / total_characters) * 100\n",
    "        print(f\"Overall Accuracy: {accuracy:.2f}%\")\n",
    "    elif images_processed > 0:\n",
    "        print(\"No valid license plate regions detected, cannot calculate accuracy.\")\n",
    "    else:\n",
    "        print(\"No images processed, cannot calculate accuracy.\")\n",
    "\n",
    "# Example usage\n",
    "test_set_path = 'C:\\\\Users\\\\Vignesh T\\\\Downloads\\\\test\\\\test\\\\test'\n",
    "test_set_annotations_file = \"C:\\\\Users\\\\Vignesh T\\\\Downloads\\\\test_labels.csv\"\n",
    "\n",
    "end_to_end_evaluation(test_set_path, test_set_annotations_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
